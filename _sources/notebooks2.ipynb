{"cells":[{"cell_type":"markdown","source":["# **Tugas 2 | Web Scraping (https://pta.trunojoyo.ac.id/)**"],"metadata":{"id":"5D2Jc4cifRqT"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"MK22lb2gAQoJ","executionInfo":{"status":"ok","timestamp":1677091854839,"user_tz":-420,"elapsed":7,"user":{"displayName":"Caca Erha","userId":"13359221303846732984"}}},"outputs":[],"source":["from urllib.request import urlopen\n","from bs4 import BeautifulSoup"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"oExHr6WnBZ9Z","executionInfo":{"status":"ok","timestamp":1677091856922,"user_tz":-420,"elapsed":2089,"user":{"displayName":"Caca Erha","userId":"13359221303846732984"}}},"outputs":[],"source":["wiki_link = \"https://pta.trunojoyo.ac.id/c_search/byprod/10\"\n","html = urlopen(wiki_link).read()\n","soup = BeautifulSoup(html, 'html.parser')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"u1MPOy2wBnF5","executionInfo":{"status":"ok","timestamp":1677091856922,"user_tz":-420,"elapsed":9,"user":{"displayName":"Caca Erha","userId":"13359221303846732984"}}},"outputs":[],"source":["content_jurnal = soup.find(\"div\", {\"id\": \"content_journal\"})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZAfw65NoC1EO"},"outputs":[],"source":["# inisialisasi variable\n","page = 0\n","temp = []\n","index = 0\n","# get link pagenation\n","for each in content_jurnal.findAll(\"li\"):\n","    bb = each.find_all(\"a\")\n","    for i in bb:\n","      try:\n","        if(\"https://pta.trunojoyo.ac.id/c_search/byprod/\" in i['href']):\n","          page = int(i['href'].replace('https://pta.trunojoyo.ac.id/c_search/byprod/10/',\"\"))\n","      except:\n","        continue\n","# action to get data by link pagenation\n","for i in range(page):\n","  page_link = \"https://pta.trunojoyo.ac.id/c_search/byprod/10/\"+str(i)\n","  html_page = urlopen(page_link).read()\n","  soup_page = BeautifulSoup(html_page, 'html.parser')\n","  content_page = soup_page.find(\"div\", {\"id\": \"content_journal\"})\n","  for page in content_page.findAll(\"li\"):\n","      page_data = page.find_all(\"a\")\n","      # action to get content in page\n","      for j in page_data:\n","        # print(j)\n","        try:\n","          if(\"https://pta.trunojoyo.ac.id/welcome/detail/\" in j['href']):\n","            page_content = j['href']\n","            html_content = urlopen(page_content).read()\n","            soup_content = BeautifulSoup(html_content, 'html.parser')\n","            data_content = soup_content.find(\"div\", {\"id\": \"content_journal\"})\n","            for data in data_content.findAll(\"li\"):\n","              # get title\n","              alist = data.find_all(\"a\")\n","              for a in alist:\n","                if('title' in a['class']):\n","                    temp_text = ''\n","                    for text in a.text.split():\n","                      if text != \" \" or text != \"\\r\" or text != \"\\n\":\n","                        temp_text+=text+\" \"\n","                    temp.append([temp_text])\n","              \n","              # get penulis\n","              spanlist= data.find_all(\"span\")\n","              temp_penulis = \"\"\n","              for span in spanlist:\n","                if(\"Penulis : \" in span.text):                  \n","                  temp_penulis = span.text.replace(\"Penulis : \",\"\")                    \n","                  temp[index].append(temp_penulis)                \n","                \n","              # get abstrak\n","              abstrak_list = data.find_all(\"p\")\n","              temp_abstrak = []\n","              for abstrak in abstrak_list:\n","                if(len(temp_abstrak) ==1):\n","                  continue\n","                else:\n","                  temp_abstrak.append(abstrak.text)\n","\n","              temp_abstrak_text = \"\"          \n","              for text_abstrak in temp_abstrak[0].split():\n","                if text_abstrak != \" \" or text_abstrak != \"\\r\" or text_abstrak != \"\\n\":\n","                        temp_abstrak_text+=text_abstrak+\" \"\n","                else:\n","                        temp_abstrak_text+=text_abstrak+\" \"\n","              temp[index].append(temp_abstrak_text)\n","              # print(len(temp))\n","              index+=1\n","                    \n","        except:\n","          continue\n","print(temp)\n","print(len(temp))"]},{"cell_type":"code","source":["# list to dataframe\n","import pandas as pd\n","import os\n","path = \"/content/drive/MyDrive/prosaindata/DataCrawling/Data_pta.csv\"\n","df = pd.DataFrame(temp, columns =['Judul', 'Penulis','Abstrak'])\n","# add column\n","df['Program Studi'] = \"Teknik Informatika\"\n","# save to csv\n","if( os.path.exists(path) == True):\n","    os.remove(path)\n","df.to_csv(path)\n","df"],"metadata":{"id":"nvM9yQEFe0BV"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1EqnMQpWM3rLdNpuAMyXZsVGkrR1PBSD9","authorship_tag":"ABX9TyOE3XX0/swJlaOiAOLbcaOu"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}